{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjvwuOmhA6BQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD4QuYZ2CXda",
        "outputId": "f226a078-1123-4b93-b86d-b1e3cb70f037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows loaded: 234\n",
            "Dataset summary:\n",
            "- time_start: 1.00007197\n",
            "- time_end: 39.007683572763\n",
            "- duration: 38.007611602763\n",
            "- packets: 234\n",
            "- unique_senders: 6\n",
            "- receiver: 10.0.0.5 (sinkNode)\n",
            "- delay_min_s: 3.711e-05\n",
            "- delay_max_s: 0.018488538952\n",
            "- delay_mean_s: 0.002964663917376068\n",
            "- delay_median_s: 0.001741685324\n",
            "\n",
            "No duplicate (Sender, Packet ID) pairs detected as retransmissions proxy.\n",
            "\n",
            "Per-sender potential retransmission-related outliers:\n",
            "   Sender  outlier_count  outlier_threshold_s  max_delay_s\n",
            " 10.0.0.1              0             0.014790     0.009652\n",
            "10.0.0.13              0             0.000085     0.000085\n",
            "10.0.0.17              0             0.000099     0.000099\n",
            " 10.0.0.2              0             0.016715     0.014093\n",
            " 10.0.0.3              0             0.019849     0.018489\n",
            " 10.0.0.9              0             0.000072     0.000072\n",
            "\n",
            "Per-1s bin stats:\n",
            " time_bin_s  packets  delay_mean_s  delay_std_s  delay_p95_s\n",
            "          1        6      0.007009     0.008132     0.017390\n",
            "          2        6      0.002829     0.003282     0.007061\n",
            "          3        6      0.002776     0.003220     0.006931\n",
            "          4        6      0.002822     0.003292     0.007111\n",
            "          5        6      0.003797     0.004329     0.009190\n",
            "          6        6      0.002816     0.003318     0.007191\n",
            "          7        6      0.002779     0.003289     0.007146\n",
            "          8        6      0.002916     0.003368     0.007226\n",
            "          9        6      0.002702     0.003156     0.006821\n",
            "         10        6      0.002812     0.003302     0.007151\n",
            "         11        6      0.002812     0.003289     0.007111\n",
            "         12        6      0.002912     0.003378     0.007261\n",
            "         13        6      0.002792     0.003242     0.006981\n",
            "         14        6      0.002782     0.003267     0.007056\n",
            "         15        6      0.002782     0.003244     0.007006\n",
            "         16        6      0.002799     0.003268     0.007046\n",
            "         17        6      0.002832     0.003292     0.007086\n",
            "         18        6      0.002886     0.003339     0.007171\n",
            "         19        6      0.002779     0.003256     0.007046\n",
            "         20        6      0.002912     0.003371     0.007241\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Google Colab notebook: Analyze udpPacketTransmissionInfo.csv (per-packet E2E delay)\n",
        "# - Computes per-sender stats (mean/median/std/jitter), per-run summary, and retransmission impact proxy\n",
        "# - Visualizes delay over time and by sender (optional if matplotlib available)\n",
        "\n",
        "# =============================\n",
        "# 0) Setup\n",
        "# =============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Optional plots\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (10, 4)\n",
        "\n",
        "# =============================\n",
        "# 1) Load CSV\n",
        "# =============================\n",
        "# If the file is already in Colab's working directory, set this to the filename.\n",
        "CSV_PATH = 'udpPacketTransmissionInfo.csv'  # overwrite per run as noted\n",
        "\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    from google.colab import files\n",
        "    print('Upload udpPacketTransmissionInfo.csv')\n",
        "    uploaded = files.upload()\n",
        "    CSV_PATH = list(uploaded.keys())\n",
        "\n",
        "# The CSV generated by simulation may contain lines with ellipses or merged rows.\n",
        "# We'll use python engine and on_bad_lines='skip' to be robust.\n",
        "df = pd.read_csv(CSV_PATH, engine='python', on_bad_lines='skip')\n",
        "\n",
        "# Normalize column names\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "expected_cols = ['Simulation Time', 'Sender', 'Receiver', 'Packet ID', 'Delay']\n",
        "missing = set(expected_cols) - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f'Missing expected columns: {missing}. Found columns: {df.columns.tolist()}')\n",
        "\n",
        "# Coerce types\n",
        "for col in ['Simulation Time', 'Delay']:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "df['Packet ID'] = pd.to_numeric(df['Packet ID'], errors='coerce')\n",
        "df = df.dropna(subset=['Simulation Time', 'Delay', 'Packet ID']).sort_values(['Simulation Time','Packet ID'])\n",
        "\n",
        "print('Rows loaded:', len(df))\n",
        "df.head()\n",
        "\n",
        "# =============================\n",
        "# 2) Basic dataset summary\n",
        "# =============================\n",
        "summary = {\n",
        "    'time_start': df['Simulation Time'].min(),\n",
        "    'time_end': df['Simulation Time'].max(),\n",
        "    'duration': df['Simulation Time'].max() - df['Simulation Time'].min(),\n",
        "    'packets': len(df),\n",
        "    'unique_senders': df['Sender'].nunique(),\n",
        "    'receiver': ','.join(sorted(df['Receiver'].astype(str).unique())),\n",
        "    'delay_min_s': df['Delay'].min(),\n",
        "    'delay_max_s': df['Delay'].max(),\n",
        "    'delay_mean_s': df['Delay'].mean(),\n",
        "    'delay_median_s': df['Delay'].median(),\n",
        "}\n",
        "print('Dataset summary:')\n",
        "for k,v in summary.items():\n",
        "    print(f'- {k}: {v}')\n",
        "\n",
        "# =============================\n",
        "# 3) Per-sender delay stats and jitter\n",
        "# =============================\n",
        "# Jitter (RFC 3550 style approximation): mean absolute inter-packet delay variation per sender.\n",
        "# Compute inter-arrival differences in delay per sender over time.\n",
        "\n",
        "def compute_jitter(g):\n",
        "    g = g.sort_values('Simulation Time')\n",
        "    if len(g) < 2:\n",
        "        return np.nan\n",
        "    delta = g['Delay'].diff().abs()\n",
        "    return delta.dropna().mean()\n",
        "\n",
        "per_sender = df.groupby('Sender').agg(\n",
        "    packets=('Packet ID','count'),\n",
        "    delay_min_s=('Delay','min'),\n",
        "    delay_max_s=('Delay','max'),\n",
        "    delay_mean_s=('Delay','mean'),\n",
        "    delay_median_s=('Delay','median'),\n",
        "    delay_std_s=('Delay','std'),\n",
        "    jitter_mean_abs_s=('Delay', lambda s: compute_jitter(df.loc[s.index]))\n",
        ").reset_index().sort_values('delay_mean_s')\n",
        "\n",
        "#print('\\nPer-sender stats:')\n",
        "#print(per_sender.to_string(index=False))\n",
        "\n",
        "# =============================\n",
        "# 4) Retransmission impact proxy\n",
        "# =============================\n",
        "# The CSV logs successful packet arrivals with their E2E delay. It typically does not explicitly mark retransmissions.\n",
        "# A practical proxy is to look for multiple arrivals with the same Packet ID from the same Sender, or unusually large delays.\n",
        "\n",
        "# Detect duplicate (Sender, Packet ID) pairs which could indicate retransmits of the same logical packet id scheme.\n",
        "duplicates = (df.duplicated(subset=['Sender','Packet ID'], keep=False))\n",
        "dup_df = df[duplicates].sort_values(['Sender','Packet ID','Simulation Time'])\n",
        "\n",
        "if dup_df.empty:\n",
        "    print('\\nNo duplicate (Sender, Packet ID) pairs detected as retransmissions proxy.')\n",
        "else:\n",
        "    print('\\nPotential retransmissions detected (same Sender & Packet ID seen multiple times):')\n",
        "    print(dup_df.to_string(index=False))\n",
        "\n",
        "# Additionally, mark outlier delays (e.g., > median + 3*IQR) per sender which might be consistent with retransmission delays.\n",
        "per_sender_outliers = []\n",
        "for sender, g in df.groupby('Sender'):\n",
        "    q1 = g['Delay'].quantile(0.25)\n",
        "    q3 = g['Delay'].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    thresh = q3 + 3*iqr if iqr > 0 else g['Delay'].max()\n",
        "    outliers = g[g['Delay'] > thresh]\n",
        "    per_sender_outliers.append({\n",
        "        'Sender': sender,\n",
        "        'outlier_count': len(outliers),\n",
        "        'outlier_threshold_s': float(thresh),\n",
        "        'max_delay_s': float(g['Delay'].max())\n",
        "    })\n",
        "per_sender_outliers = pd.DataFrame(per_sender_outliers)\n",
        "print('\\nPer-sender potential retransmission-related outliers:')\n",
        "print(per_sender_outliers.to_string(index=False))\n",
        "\n",
        "# =============================\n",
        "# 5) Time-binned delay distribution\n",
        "# =============================\n",
        "# Bin by 1-second windows to see whether mean/variance of delay increases over time.\n",
        "\n",
        "df['time_bin_s'] = np.floor(df['Simulation Time']).astype(int)\n",
        "by_bin = df.groupby('time_bin_s').agg(\n",
        "    packets=('Packet ID','count'),\n",
        "    delay_mean_s=('Delay','mean'),\n",
        "    delay_std_s=('Delay','std'),\n",
        "    delay_p95_s=('Delay', lambda s: s.quantile(0.95)),\n",
        ").reset_index()\n",
        "print('\\nPer-1s bin stats:')\n",
        "print(by_bin.head(20).to_string(index=False))\n",
        "\n",
        "# =============================\n",
        "# 6) Visualizations (optional)\n",
        "# =============================\n",
        "try:\n",
        "    # Delay over time\n",
        "    plt.figure()\n",
        "    plt.plot(df['Simulation Time'], df['Delay'], '.', alpha=0.5)\n",
        "    plt.xlabel('Simulation Time (s)')\n",
        "    plt.ylabel('End-to-End Delay (s)')\n",
        "    plt.title('Per-packet E2E Delay over Time')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Delay by sender (boxplot)\n",
        "    plt.figure()\n",
        "    order = df.groupby('Sender')['Delay'].median().sort_values().index\n",
        "    df.boxplot(column='Delay', by='Sender', positions=np.arange(len(order)),\n",
        "               widths=0.6, grid=False)\n",
        "    plt.xticks(range(len(order)), order, rotation=45, ha='right')\n",
        "    plt.suptitle('')\n",
        "    plt.title('Delay by Sender')\n",
        "    plt.ylabel('Delay (s)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Bin trends\n",
        "    plt.figure()\n",
        "    plt.plot(by_bin['time_bin_s'], by_bin['delay_mean_s'], label='Mean')\n",
        "    plt.plot(by_bin['time_bin_s'], by_bin['delay_p95_s'], label='P95')\n",
        "    plt.xlabel('Time bin (s)')\n",
        "    plt.ylabel('Delay (s)')\n",
        "    plt.title('Delay Trend per 1s Bin')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print('Plotting skipped or failed:', e)\n",
        "\n",
        "# =============================\n",
        "# 7) Save key outputs as CSVs\n",
        "# =============================\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v_WolgfTrKW",
        "outputId": "bb83c001-1a6d-4cc3-c880-b45776f7e74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-node MAC-layer retransmission proxy metrics:\n",
            "           Node  cw_min  raw_stepups  retransmission_episodes  estimated_attempts  retransmission_rate_per_sec  observation_time_s\n",
            "    accessPoint      31            0                        0                   1                     0.000000           39.900000\n",
            "wirelessHost[0]      15           42                       39                  40                     0.980341           39.782060\n",
            "wirelessHost[1]      15           40                       39                  40                     0.981769           39.724211\n",
            "wirelessHost[2]      15           43                       39                  40                     0.981798           39.723021\n",
            "\n",
            "Overall proxies:\n",
            "- nodes: 4\n",
            "- total_retransmission_episodes: 117\n",
            "- total_raw_stepups: 125\n",
            "- total_estimated_attempts: 121\n",
            "- retransmission_fraction: 0.9669421487603306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2453899632.py:85: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  g['was_min_prev'] = g['is_min'].shift(1).fillna(False)\n",
            "/tmp/ipython-input-2453899632.py:85: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  g['was_min_prev'] = g['is_min'].shift(1).fillna(False)\n",
            "/tmp/ipython-input-2453899632.py:85: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  g['was_min_prev'] = g['is_min'].shift(1).fillna(False)\n",
            "/tmp/ipython-input-2453899632.py:85: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  g['was_min_prev'] = g['is_min'].shift(1).fillna(False)\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Google Colab: Estimate MAC-layer retransmission rate from cwUsed.csv\n",
        "# Assumptions (typical 802.11-style CW progression):\n",
        "# - CW starts at a minimum (e.g., 15) for a new attempt.\n",
        "# - Upon failed transmission (collision/corruption), CW doubles up to a max (e.g., 31, 63, 127, ... depending on config).\n",
        "# - Each step-up event indicates at least one retransmission attempt was needed.\n",
        "# This script treats increases in CW for a node within a short time window as retransmission evidence and estimates rate.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "CSV_PATH = 'cwUsed.csv'\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print('Upload cwUsed.csv')\n",
        "        uploaded = files.upload()\n",
        "        CSV_PATH = list(uploaded.keys())\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Load CSV (robust to minor formatting issues)\n",
        "df = pd.read_csv(CSV_PATH, engine='python', on_bad_lines='skip')\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "# Some files may have a typo like 'Contetion Size' instead of 'Contention Size'\n",
        "if 'Contention Size' in df.columns:\n",
        "    cw_col = 'Contention Size'\n",
        "elif 'Contetion Size' in df.columns:\n",
        "    cw_col = 'Contetion Size'\n",
        "else:\n",
        "    raise ValueError('Missing contention window column (expected Contention Size or Contetion Size). Found: ' + str(df.columns))\n",
        "\n",
        "# Coerce types and clean\n",
        "num_cols = ['Simulation Time']\n",
        "for c in num_cols:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "df[cw_col] = pd.to_numeric(df[cw_col], errors='coerce')\n",
        "df = df.dropna(subset=['Simulation Time', 'Node', cw_col]).sort_values(['Node','Simulation Time'])\n",
        "\n",
        "# Identify CW step-up events per node (proxy for retransmissions)\n",
        "# A step-up occurs when CW(t) > CW(t-1) for the same node.\n",
        "# Optional: collapse rapid toggles within a small window as a single episode.\n",
        "MERGE_WINDOW_S = 0.01  # merge CW increases occurring within 10 ms as one backoff episode\n",
        "\n",
        "results = []\n",
        "\n",
        "for node, g in df.groupby('Node'):\n",
        "    g = g.sort_values('Simulation Time').copy()\n",
        "    g['cw_prev'] = g[cw_col].shift(1)\n",
        "    g['t_prev'] = g['Simulation Time'].shift(1)\n",
        "    g['step_up'] = (g[cw_col] > g['cw_prev'])\n",
        "\n",
        "    # Count raw step-ups\n",
        "    stepups = g[g['step_up']].copy()\n",
        "\n",
        "    # Merge close step-ups into episodes\n",
        "    episodes = []\n",
        "    current = None\n",
        "    for _, r in stepups.iterrows():\n",
        "        t = r['Simulation Time']\n",
        "        if current is None:\n",
        "            current = {'start': t, 'end': t, 'count': 1}\n",
        "        else:\n",
        "            if t - current['end'] <= MERGE_WINDOW_S:\n",
        "                current['end'] = t\n",
        "                current['count'] += 1\n",
        "            else:\n",
        "                episodes.append(current)\n",
        "                current = {'start': t, 'end': t, 'count': 1}\n",
        "    if current is not None:\n",
        "        episodes.append(current)\n",
        "\n",
        "    # Compute node-level metrics\n",
        "    total_time = g['Simulation Time'].max() - g['Simulation Time'].min() if len(g) > 1 else np.nan\n",
        "    retransmissions = len(episodes)  # episodes as retransmission occurrences\n",
        "    raw_stepups = len(stepups)\n",
        "\n",
        "    # Estimate number of attempts: count times CW returns to minimum after being larger\n",
        "    cw_min = g[cw_col].min()\n",
        "    # A new TX attempt can be proxied by transitions to cw_min from a higher CW, or any record at cw_min following non-cw_min\n",
        "    g['is_min'] = (g[cw_col] == cw_min)\n",
        "    g['was_min_prev'] = g['is_min'].shift(1).fillna(False)\n",
        "    g['was_higher_prev'] = (g['cw_prev'] > cw_min).fillna(False)\n",
        "    new_attempts = int(((g['is_min'] & (g['was_higher_prev'] | (~g['was_min_prev']))).sum()))\n",
        "    # Fallback if too low: also consider each second boundary min as new attempt if logs are periodic\n",
        "    if new_attempts == 0:\n",
        "        # count unique integer-second bins with cw at min as attempts\n",
        "        bins = np.floor(g.loc[g['is_min'], 'Simulation Time']).astype(int).nunique()\n",
        "        new_attempts = int(bins)\n",
        "\n",
        "    rate_per_sec = (retransmissions / total_time) if total_time and total_time > 0 else np.nan\n",
        "\n",
        "    results.append({\n",
        "        'Node': node,\n",
        "        'cw_min': int(cw_min),\n",
        "        'raw_stepups': int(raw_stepups),\n",
        "        'retransmission_episodes': int(retransmissions),\n",
        "        'estimated_attempts': int(new_attempts),\n",
        "        'retransmission_rate_per_sec': float(rate_per_sec),\n",
        "        'observation_time_s': float(total_time),\n",
        "    })\n",
        "\n",
        "res = pd.DataFrame(results).sort_values('Node')\n",
        "print('Per-node MAC-layer retransmission proxy metrics:')\n",
        "print(res.to_string(index=False))\n",
        "\n",
        "# Aggregate overall metrics\n",
        "overall = {\n",
        "    'nodes': res['Node'].nunique(),\n",
        "    'total_retransmission_episodes': int(res['retransmission_episodes'].sum()),\n",
        "    'total_raw_stepups': int(res['raw_stepups'].sum()),\n",
        "    'total_estimated_attempts': int(res['estimated_attempts'].sum()),\n",
        "}\n",
        "# Overall retransmission fraction = episodes / attempts (proxy)\n",
        "if overall['total_estimated_attempts'] > 0:\n",
        "    overall['retransmission_fraction'] = overall['total_retransmission_episodes'] / overall['total_estimated_attempts']\n",
        "else:\n",
        "    overall['retransmission_fraction'] = np.nan\n",
        "\n",
        "print('\\nOverall proxies:')\n",
        "for k,v in overall.items():\n",
        "    print(f'- {k}: {v}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v0Rl6WBp6NQ",
        "outputId": "b11861f5-90b0-4550-9fc1-9883a5eaca9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Overall stats (all nodes):\n",
            "- header_ber_min: 0.0\n",
            "- header_ber_median: 0.0\n",
            "- header_ber_avg: 0.01662842459241557\n",
            "- header_ber_max: 0.879462\n",
            "- data_ber_min: 0.0\n",
            "- data_ber_median: 0.0\n",
            "- data_ber_avg: 0.02007427638280019\n",
            "- data_ber_max: 1.0\n",
            "- packet_ber_min: 0.0\n",
            "- packet_ber_median: 0.0\n",
            "- packet_ber_avg: 0.01994980745288146\n",
            "- packet_ber_max: 0.9957457176470588\n",
            "- snir_min: 1.22529\n",
            "- snir_median: 700.871\n",
            "- snir_avg: 926.3193498170732\n",
            "- snir_max: 2481.18\n",
            "- packets: 2132.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Input/output filenames\n",
        "HEADER_FILE = 'HeaderErrorRate.csv'\n",
        "DATA_FILE = 'DataErrorRate.csv'\n",
        "OUT_MERGED = 'PacketErrorRates_Merged.csv'\n",
        "OUT_SUMMARY = 'PacketErrorRates_Summary.csv'         # per-node BER + SNIR stats (min/median/mean/max)\n",
        "OUT_SNR_SUMMARY = 'SNIR_By_Node_Summary.csv'         # kept for compatibility; now same SNIR stats subset\n",
        "OUT_OVERALL = 'PacketErrorRates_Overall.csv'         # overall (all nodes combined) stats\n",
        "\n",
        "# Load input CSVs\n",
        "header_df = pd.read_csv(HEADER_FILE)\n",
        "data_df = pd.read_csv(DATA_FILE)\n",
        "\n",
        "# Clean/normalize headers\n",
        "header_df.columns = [c.strip().replace('\\u00a0',' ').replace('\\ufeff','') for c in header_df.columns]\n",
        "data_df.columns = [c.strip().replace('\\u00a0',' ').replace('\\ufeff','') for c in data_df.columns]\n",
        "\n",
        "# Rename for clarity\n",
        "header_df = header_df.rename(columns={\n",
        "    'Header Length':'HeaderLength',\n",
        "    'Error rate':'HeaderBER'\n",
        "})\n",
        "data_df = data_df.rename(columns={\n",
        "    'Data Length':'DataLength',\n",
        "    'Error rate':'DataBER'\n",
        "})\n",
        "\n",
        "# Ensure numeric types\n",
        "for col in ['Simulation Time','SNIR','HeaderLength','HeaderBER']:\n",
        "    if col in header_df.columns:\n",
        "        header_df[col] = pd.to_numeric(header_df[col], errors='coerce')\n",
        "for col in ['Simulation Time','SNIR','DataLength','DataBER']:\n",
        "    if col in data_df.columns:\n",
        "        data_df[col] = pd.to_numeric(data_df[col], errors='coerce')\n",
        "\n",
        "# Drop exact duplicate lines to avoid double counting\n",
        "header_df = header_df.drop_duplicates(subset=['Simulation Time','Node','HeaderLength','HeaderBER','SNIR'])\n",
        "data_df = data_df.drop_duplicates(subset=['Simulation Time','Node','DataLength','DataBER','SNIR'])\n",
        "\n",
        "# Pair header and data by Simulation Time + Node + SNIR\n",
        "merged = pd.merge_asof(\n",
        "    header_df.sort_values('Simulation Time'),\n",
        "    data_df.sort_values('Simulation Time'),\n",
        "    on='Simulation Time',\n",
        "    by=['Node','SNIR'],\n",
        "    direction='nearest',\n",
        "    tolerance=1e-9\n",
        ")\n",
        "\n",
        "# Keep only fully matched rows\n",
        "merged = merged.dropna(subset=['HeaderLength','DataLength','HeaderBER','DataBER','SNIR'])\n",
        "\n",
        "# Compute total bits and packet-level BER as length-weighted average\n",
        "merged['TotalBits'] = merged['HeaderLength'] + merged['DataLength']\n",
        "merged = merged[merged['TotalBits'] > 0]\n",
        "merged['PacketBER'] = (merged['HeaderBER']*merged['HeaderLength'] + merged['DataBER']*merged['DataLength']) / merged['TotalBits']\n",
        "\n",
        "# Reorder and sort\n",
        "cols = ['Simulation Time','Node','SNIR','HeaderLength','HeaderBER','DataLength','DataBER','TotalBits','PacketBER']\n",
        "merged = merged[cols].sort_values(['Simulation Time','Node']).reset_index(drop=True)\n",
        "\n",
        "# Write detailed merged per-packet CSV\n",
        "merged.to_csv(OUT_MERGED, index=False)\n",
        "\n",
        "# Helper to build min/median/mean/max spec for a column with a prefix\n",
        "def stat_block(col, prefix):\n",
        "    return {\n",
        "        f'{prefix}_min': (col, 'min'),\n",
        "        f'{prefix}_median': (col, 'median'),\n",
        "        f'{prefix}_avg': (col, 'mean'),\n",
        "        f'{prefix}_max': (col, 'max'),\n",
        "    }\n",
        "\n",
        "# Per-node metrics: BER (header/data/packet) + SNIR\n",
        "agg_spec = {}\n",
        "agg_spec.update(stat_block('HeaderBER', 'header_ber'))\n",
        "agg_spec.update(stat_block('DataBER', 'data_ber'))\n",
        "agg_spec.update(stat_block('PacketBER', 'packet_ber'))\n",
        "agg_spec.update(stat_block('SNIR', 'snir'))\n",
        "\n",
        "summary = merged.groupby('Node').agg(**agg_spec)\n",
        "\n",
        "# Optional: also include packet counts per node\n",
        "summary['packets'] = merged.groupby('Node')['PacketBER'].size()\n",
        "\n",
        "# Save per-node summary\n",
        "summary.to_csv(OUT_SUMMARY)\n",
        "\n",
        "# Maintain the SNIR-only summary file for compatibility, now min/median/avg/max\n",
        "snr_summary = merged.groupby('Node').agg(\n",
        "    min_snir=('SNIR','min'),\n",
        "    median_snir=('SNIR','median'),\n",
        "    avg_snir=('SNIR','mean'),\n",
        "    max_snir=('SNIR','max'),\n",
        "    packets=('SNIR','size')\n",
        ")\n",
        "snr_summary.to_csv(OUT_SNR_SUMMARY)\n",
        "\n",
        "# Overall (all nodes combined) stats across entire dataset\n",
        "overall = pd.DataFrame({\n",
        "    'header_ber_min': [merged['HeaderBER'].min()],\n",
        "    'header_ber_median': [merged['HeaderBER'].median()],\n",
        "    'header_ber_avg': [merged['HeaderBER'].mean()],\n",
        "    'header_ber_max': [merged['HeaderBER'].max()],\n",
        "    'data_ber_min': [merged['DataBER'].min()],\n",
        "    'data_ber_median': [merged['DataBER'].median()],\n",
        "    'data_ber_avg': [merged['DataBER'].mean()],\n",
        "    'data_ber_max': [merged['DataBER'].max()],\n",
        "    'packet_ber_min': [merged['PacketBER'].min()],\n",
        "    'packet_ber_median': [merged['PacketBER'].median()],\n",
        "    'packet_ber_avg': [merged['PacketBER'].mean()],\n",
        "    'packet_ber_max': [merged['PacketBER'].max()],\n",
        "    'snir_min': [merged['SNIR'].min()],\n",
        "    'snir_median': [merged['SNIR'].median()],\n",
        "    'snir_avg': [merged['SNIR'].mean()],\n",
        "    'snir_max': [merged['SNIR'].max()],\n",
        "    'packets': [len(merged)]\n",
        "})\n",
        "overall.to_csv(OUT_OVERALL, index=False)\n",
        "\n",
        "# print('Per-node packet-level BER and SNIR stats (min/median/avg/max):')\n",
        "# for index, row in summary.iterrows():\n",
        "#     print(f\"\\nNode: {index}\")\n",
        "#     for col, value in row.items():\n",
        "#         print(f\"- {col}: {value}\")\n",
        "\n",
        "print('\\nOverall stats (all nodes):')\n",
        "for index, row in overall.iterrows():\n",
        "    for col, value in row.items():\n",
        "        print(f\"- {col}: {value}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
